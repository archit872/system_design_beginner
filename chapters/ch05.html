<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Ch. 5 — Scalability & Load Handling</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Scale up vs. out, load balancers and algorithms, stateless vs. stateful services, and a practical method to find and fix bottlenecks.">
  <meta property="og:title" content="Chapter 5 — Scalability & Load Handling">
  <meta property="og:description" content="Understand horizontal vs. vertical scaling, load balancing strategies, statelessness, and bottleneck diagnosis with hands-on practice.">
  <base href="../">
  <link rel="stylesheet" href="styles/theme.css">
  <script src="scripts/app.js" defer></script>
</head>
<body>
  <a class="skip-link" href="#main">Skip to content</a>

  <!-- Canonical Navigation (copy verbatim across all pages) -->
  <nav class="app-nav">
    <div class="container inner">
      <div class="brand">System Design (Beginner)</div>
      <button class="toggle js-nav-toggle" aria-expanded="false" aria-controls="site-menu" aria-label="Toggle menu">☰ Menu</button>
      <div id="site-menu" class="menu" role="navigation" aria-label="Primary">
        <a href="index.html">Home</a>
        <a href="chapters/ch01.html">Chapters</a>
        <a href="chapters/appendix.html">Appendix</a>
        <a href="chapters/glossary.html">Glossary</a>
      </div>
    </div>
  </nav>

  <header class="page-hero" id="top">
    <div class="container">
      <div class="meta">
        <span class="badge badge-primary">Chapter 5</span>
        <span class="badge">Scale & Load</span>
      </div>
      <h1>Scalability &amp; Load Handling</h1>
      <p class="abstract">Scale is about doing <em>more</em> work with <em>bounded</em> latency and cost. In this chapter you’ll contrast <abbr title="Adding CPU/RAM to a single node">vertical scaling</abbr> with <abbr title="Adding more nodes behind a load balancer">horizontal scaling</abbr>, learn common <abbr title="Components that distribute traffic across replicas">load balancer</abbr> algorithms, design <strong>stateless</strong> services that scale out cleanly, and follow a step-by-step method to locate and relieve <strong>bottlenecks</strong>.</p>
    </div>
  </header>

  <main id="main" class="container section">
    <section class="section" aria-labelledby="prereqs">
      <h2 id="prereqs">Prerequisites &amp; Learning Objectives</h2>
      <div class="grid two">
        <div class="card">
          <h3 class="mt-0">Prerequisites</h3>
          <ul>
            <li>Chapter 2 (request lifecycle) and Chapter 4 (caching basics).</li>
            <li>Basic comfort with OS metrics (CPU, memory, disk IO, network).</li>
          </ul>
        </div>
        <div class="card">
          <h3 class="mt-0">Objectives</h3>
          <ul>
            <li>Explain scale-up vs. scale-out and when each is appropriate.</li>
            <li>Describe how L4/L7 load balancers route traffic and compare algorithms.</li>
            <li>Design stateless services and manage state with databases, caches, and sticky sessions when required.</li>
            <li>Use a practical method to identify bottlenecks and plan capacity.</li>
          </ul>
        </div>
      </div>
    </section>

    <figure>
      <img src="assets/scaling-strategies.svg" alt="Side-by-side diagram of vertical scaling (bigger box) vs horizontal scaling (many small boxes behind a balancer)" />
      <figcaption>Two levers: scale up (bigger machine) vs. scale out (more machines). Most internet services favor scale out for resilience and elasticity.</figcaption>
    </figure>

    <section class="section" aria-labelledby="scaling-basics">
      <h2 id="scaling-basics">5.1 Scaling Basics: Vertical vs. Horizontal</h2>
      <p><strong>Vertical scaling</strong> increases resources (CPU/RAM/IO) on a single node. It’s simple—no distributed complexity—but has hard limits and creates a single large blast radius. It’s ideal for short-term relief, monolithic DBs that benefit from large RAM for buffer caches, or when software isn’t ready to shard.</p>
      <p><strong>Horizontal scaling</strong> adds more nodes and distributes the workload. It improves fault isolation and elasticity, but requires load balancing, distributed coordination, and consistent state management. Costs can be optimized with autoscaling and right-sizing instances or containers.</p>
      <div class="grid two">
        <div class="card">
          <h3 class="mt-0">Analogy</h3>
          <p>Vertical scale is buying a bigger oven; horizontal scale is adding more ovens and a chef to route orders. If one oven fails, dinner is ruined—unless you have many ovens.</p>
        </div>
        <div class="card">
          <h3 class="mt-0">Compare &amp; Contrast</h3>
          <table>
            <thead><tr><th>Aspect</th><th>Scale Up</th><th>Scale Out</th></tr></thead>
            <tbody>
              <tr><td>Complexity</td><td>Low</td><td>Medium–High</td></tr>
              <tr><td>Ceiling</td><td>Hardware-limited</td><td>Near-linear (with caveats)</td></tr>
              <tr><td>Resilience</td><td>Single point of failure</td><td>Replica failures tolerated</td></tr>
              <tr><td>Cost curve</td><td>Steep at high end</td><td>Granular; pay-for-usage</td></tr>
            </tbody>
          </table>
        </div>
      </div>
      <div class="callout warn">
        <span class="icon">⚖️</span>
        <strong>Trade-off:</strong> The more you scale out, the more you pay the coordination tax (timeouts, retries, distributed locks). Don’t distribute what you don’t have to.
      </div>
    </section>

    <section class="section" aria-labelledby="load-balancers">
      <h2 id="load-balancers">5.2 Load Balancers Explained</h2>
      <p>A <strong>load balancer (LB)</strong> sits in front of a fleet and distributes traffic across healthy replicas. L4 balancers operate on TCP/UDP connections (faster, simpler); L7 balancers understand HTTP semantics (paths, headers) and can route by URL, cookie, or host.</p>

      <figure>
        <img src="assets/load-balancer-algorithms.svg" alt="Diagram showing round-robin, least-connections, and consistent-hashing request distribution to backend servers" />
        <figcaption>Common algorithms: <em>round robin</em> rotates sequentially, <em>least connections</em> favors less-busy nodes, <em>consistent hashing</em> keeps a user’s requests on the same shard.</figcaption>
      </figure>

      <div class="grid two">
        <div class="card">
          <h3 class="mt-0">Algorithms</h3>
          <ul>
            <li><strong>Round Robin:</strong> Simple and fair when requests are uniform.</li>
            <li><strong>Least Connections:</strong> Better when request durations vary widely.</li>
            <li><strong>Weighted:</strong> Skew traffic to beefier nodes or away from draining ones.</li>
            <li><strong>Consistent Hashing:</strong> Route by key (user id) to preserve cache locality; enables simple sharding.</li>
          </ul>
        </div>
        <div class="card">
          <h3 class="mt-0">Health &amp; Resilience</h3>
          <ul>
            <li>Active health checks (HTTP 200, latency budgets, TLS validity).</li>
            <li>Connection draining on deploys; outlier detection to eject bad replicas.</li>
            <li>Rate limiting and circuit breaking at the edge to protect origins.</li>
          </ul>
        </div>
      </div>

      <div class="callout info">
        <strong>Edge vs. Internal:</strong> A CDN or reverse proxy may act as the first LB at the edge (geo-aware), while an internal LB fans out to service pods inside a VPC/cluster.
      </div>
    </section>

    <section class="section" aria-labelledby="stateless">
      <h2 id="stateless">5.3 Stateless vs. Stateful Services</h2>
      <p><strong>Stateless services</strong> keep no user session or request history in memory between calls; all state lives in shared data stores (DB/cache/object storage). This allows you to add or remove replicas freely and replace instances during deploys without cutting users off.</p>
      <p><strong>Stateful services</strong> retain session-specific or partition-specific state (e.g., in-memory rooms for a WebSocket chat, primary DB nodes). They often require <em>stickiness</em> (route a user to the same node) or a <em>consistency protocol</em> to move/replicate state safely.</p>

      <div class="grid two">
        <figure>
          <img src="assets/stateless-vs-stateful.svg" alt="Two diagrams: stateless service with shared cache/db; stateful service with sticky sessions and shard ownership" />
          <figcaption>Stateless scales via replicas; stateful requires ownership, stickiness, or partitioning.</figcaption>
        </figure>
        <div class="card">
          <h3 class="mt-0">Design Tips</h3>
          <ul>
            <li>Prefer stateless application tiers; externalize sessions to Redis or signed tokens (JWT).</li>
            <li>For stateful workloads, define <em>ownership</em> (which node owns which shard/room) and a plan for failover.</li>
            <li>Make mutating operations <strong>idempotent</strong> to tolerate retries.</li>
          </ul>
        </div>
      </div>

      <div class="callout warn">
        <strong>Trade-off:</strong> Sticky sessions simplify state but reduce elasticity and complicate autoscaling. External session stores add latency but restore flexibility.
      </div>
    </section>

    <section class="section" aria-labelledby="bottlenecks">
      <h2 id="bottlenecks">5.4 Identifying &amp; Resolving Bottlenecks</h2>
      <p>A system’s <strong>throughput</strong> is limited by its slowest stage. Finding that stage requires measurement, not guesswork. Use this loop:</p>

      <figure>
        <img src="assets/bottleneck-method.svg" alt="Loop diagram: Measure → Hypothesize → Isolate → Fix → Verify → Repeat" />
        <figcaption>Performance loop: measure before and after, change one variable at a time, and verify improvement at p95/p99 not just average.</figcaption>
      </figure>

      <ol>
        <li><strong>Measure:</strong> Capture baseline latency percentiles and component metrics (LB, app, DB, cache). Gather request IDs and traces.</li>
        <li><strong>Hypothesize:</strong> Do histograms show queueing? Is DB CPU or IO-bound? Are caches missing?</li>
        <li><strong>Isolate:</strong> Run controlled load tests; toggle features; temporarily bypass caches to compare.</li>
        <li><strong>Fix:</strong> Apply targeted changes: add an index, precompute results, increase connection pools, or add replicas.</li>
        <li><strong>Verify:</strong> Re-run load; confirm p95/p99 improvements and absence of regressions.</li>
      </ol>

      <div class="grid two">
        <div class="card">
          <h3 class="mt-0">Typical Bottlenecks</h3>
          <ul>
            <li>Database hot rows/partitions → add composite indexes, cache, or shard by hot key.</li>
            <li>App CPU due to serialization/JSON → enable compression wisely or switch to binary on internal hops.</li>
            <li>Network egress → serve static assets via CDN; reduce payload size.</li>
          </ul>
        </div>
        <div class="card">
          <h3 class="mt-0">Capacity Planning (Back-of-Envelope)</h3>
          <p>If one replica handles <code>r</code> requests/sec at target CPU, and you expect peak <code>P</code> RPS, you need ~<code>ceil(P / (r * headroom))</code> replicas, where <code>headroom</code> (e.g., 0.6–0.7) leaves space for failure and variability.</p>
        </div>
      </div>
    </section>

    <section class="section" aria-labelledby="case">
      <h2 id="case">Case Study: Scaling a Read-Heavy API During a Flash Sale (≈200 words)</h2>
      <p>A retailer’s “product summary” API spikes 12× during a 2-hour flash sale. Baseline: 40 ms median, 600 ms p95 at 1k RPS; DB is near CPU limits. The team executes a three-step plan. <strong>(1) Edge &amp; app caching:</strong> CDN caches static images and a short-TTL JSON for anonymous users; app adds a cache-aside entry <code>product:{id}:summary</code> with 60 s TTL + jitter. <strong>(2) Scale out:</strong> App replicas increase from 6→18 behind a least-connections LB; Redis is scaled to a 3-node cluster with replication. <strong>(3) DB relief:</strong> Add a covering index for the summary fields and a read replica for non-critical queries. Guardrails include per-IP rate limits and request coalescing for cold keys.</p>
      <p>Results: Cache hit rate reaches ~85% at peak; app p95 drops to ~220 ms; DB CPU falls under 60%. When a node fails mid-sale, the LB ejects it and the fleet absorbs the load. After the sale, autoscaling returns replicas to baseline. The design balances <em>scalability</em> (replicas, caches), <em>reliability</em> (health checks, failout), and <em>maintainability</em> (simple cache keys, minimal schema change).</p>
    </section>

    <section class="section" aria-labelledby="resources">
      <h2 id="resources">Resources</h2>
      <ul class="resource-list">
        <li class="item">
          <div>
            <a class="external" href="https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/" target="_blank">NGINX — HTTP Load Balancing</a>
            <div class="meta">Algorithms, health checks, connection reuse, and configuration tips.</div>
          </div>
          <span class="badge">L7 LB</span>
        </li>
        <li class="item">
          <div>
            <a class="external" href="https://www.haproxy.com/documentation/haproxy-configuration-tutorials/load-balancing/" target="_blank">HAProxy — Load Balancing Tutorials</a>
            <div class="meta">Operational patterns for least-connections, stickiness, and draining.</div>
          </div>
          <span class="badge">L4/L7 LB</span>
        </li>
        <li class="item">
          <div>
            <a class="external" href="https://aws.amazon.com/elasticloadbalancing/features/" target="_blank">AWS Elastic Load Balancing — Features</a>
            <div class="meta">L7/L4 options, health checks, and autoscaling integrations for cloud deployments.</div>
          </div>
          <span class="badge">Cloud</span>
        </li>
      </ul>
    </section>

    <section class="section practice" aria-labelledby="practice">
      <div class="title">Practice <span class="badge">35–60 min</span></div>
      <h2 id="practice" class="mt-1">Hands-on Tasks</h2>
      <ol>
        <li><strong>Diagram a scalable web app (10–15 min).</strong> Draw: client → CDN → LB → N app replicas → cache → DB. Add health checks and autoscaling triggers. <em>Success:</em> Every box has a single responsibility and a failover story.</li>
        <li><strong>Back-of-envelope capacity (10–15 min).</strong> Given 150 RPS/replica at 60% CPU and a 2× peak safety factor, compute required replicas for 1,200 RPS. <em>Success:</em> Shows math and headroom choice.</li>
        <li><strong>Find a bottleneck (10–15 min).</strong> Run a local load test; capture p50/p95 and CPU for app vs. DB. <em>Success:</em> Identifies the slowest stage and proposes a specific fix (e.g., add index, cache).</li>
        <li><strong>Statelessness refactor (optional, 10–15 min).</strong> Move session state from memory to Redis/JWT; remove sticky sessions. <em>Success:</em> Requests succeed across rolling restarts without loss of session.</li>
      </ol>
    </section>

    <section class="section mastery" aria-labelledby="mastery">
      <div class="title">Mastery Check <span class="badge">Self-assessment</span></div>
      <h2 id="mastery" class="mt-1">Questions (with sample answers)</h2>

      <details>
        <summary>1) When is vertical scaling preferable to horizontal scaling?</summary>
        <div class="pad-2">
          <p><strong>Sample answer:</strong> When the app is not partitionable, the workload fits on a single large box, or you need quick relief without re-architecting—e.g., giving a DB more RAM to fit the working set.</p>
        </div>
      </details>

      <details>
        <summary>2) Contrast L4 vs. L7 load balancing.</summary>
        <div class="pad-2">
          <p><strong>Sample answer:</strong> L4 balances at the transport layer (TCP/UDP) and is faster but blind to HTTP details; L7 understands HTTP/HTTPS and can route by path/host/cookie, enabling canaries and A/B routing.</p>
        </div>
      </details>

      <details>
        <summary>3) Why are stateless services easier to scale?</summary>
        <div class="pad-2">
          <p><strong>Sample answer:</strong> Any replica can handle any request; you can add/remove replicas without migrating in-memory state. State moves to shared stores that can be scaled independently.</p>
        </div>
      </details>

      <details>
        <summary>4) Give an example where consistent hashing helps.</summary>
        <div class="pad-2">
          <p><strong>Sample answer:</strong> Routing a user’s session or cache key to the same shard so adding/removing nodes only remaps a small fraction of keys, preserving locality and minimizing cache churn.</p>
        </div>
      </details>

      <details>
        <summary>5) What’s a thundering herd and how can an LB mitigate it?</summary>
        <div class="pad-2">
          <p><strong>Sample answer:</strong> Many clients retry simultaneously (e.g., after a timeout). The LB can shed load, enforce jittered backoff via gateways, and favor healthy replicas while upstreams use request coalescing/caching.</p>
        </div>
      </details>

      <details>
        <summary>6) How would you decide between adding replicas versus adding a cache?</summary>
        <div class="pad-2">
          <p><strong>Sample answer:</strong> If the bottleneck is CPU-bound business logic with little data reuse, add replicas. If hot reads dominate and results are reusable, add a cache to reduce DB load and latency.</p>
        </div>
      </details>

      <details>
        <summary>7) Provide a quick capacity estimate formula.</summary>
        <div class="pad-2">
          <p><strong>Sample answer:</strong> <code>replicas ≈ ceil(peak_RPS / (RPS_per_replica × headroom))</code>, with headroom 0.6–0.7 for failures and variance.</p>
        </div>
      </details>
    </section>

    <section class="section" aria-labelledby="recap">
      <h2 id="recap">Recap &amp; Next Steps</h2>
      <p><strong>Recap:</strong> You learned when to scale up vs. out, how L4/L7 balancers distribute traffic, why statelessness unlocks elasticity, and how to methodically find/fix bottlenecks. You practiced capacity estimates and drew scalable topologies.</p>
      <p><strong>Next:</strong> In <a href="chapters/ch06.html">Chapter 6</a>, we shift from performance to <em>reliability</em>: redundancy, failover, disaster recovery, and the basics of monitoring and alerting.</p>
    </section>

    <nav class="next-prev" aria-label="Chapter pager">
      <a rel="prev" href="chapters/ch04.html">Previous: Caching &amp; Performance Basics</a>
      <a rel="next" href="chapters/ch06.html">Next: Reliability &amp; Fault Tolerance</a>
    </nav>
  </main>

  <footer class="site-footer">
    <div class="container">
      <p class="mb-0">© 2025 System Design (Beginner). Chapter 5.</p>
    </div>
  </footer>

  <!--
  CHECKLIST (do not remove)
  - [x] /styles/theme.css + /scripts/app.js linked (paths correct for /chapters/)
  - [x] <base href="../"> present & correct
  - [x] Canonical nav verbatim; active link via app.js
  - [x] Pager prev=ch04, next=ch06
  - [x] Sections meet depth: 5.1–5.4 + examples, compare/contrast, analogy, trade-offs
  - [x] Practice (3–4 tasks) with time + success criteria
  - [x] Mastery (6–7 Qs) with sample answers
  - [x] ≥3 figures: scaling-strategies.svg, load-balancer-algorithms.svg, stateless-vs-stateful.svg, bottleneck-method.svg under /assets
  - [x] ≥8 glossary-style terms via <abbr> (vertical, horizontal, load balancer, L4, L7, consistent hashing, stateless, bottleneck, throughput)
  - [x] Recap + Next steps
  -->
</body>
</html>
